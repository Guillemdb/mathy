{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file is generated from a mathy documentation code snippet.\n",
    "!pip install mathy\n",
    "\"\"\"Environment with user-defined rewards per-timestep based on the\n",
    "rule that was applied by the agent.\"\"\"\n",
    "\n",
    "from typing import List, Type\n",
    "\n",
    "from mathy import BaseRule, MathyEnv, MathyEnvState\n",
    "from mathy.rules import AssociativeSwapRule, CommutativeSwapRule\n",
    "\n",
    "\n",
    "class CustomTimestepRewards(MathyEnv):\n",
    "    def get_rewarding_actions(self, state: MathyEnvState) -> List[Type[BaseRule]]:\n",
    "        return [AssociativeSwapRule]\n",
    "\n",
    "    def get_penalizing_actions(self, state: MathyEnvState) -> List[Type[BaseRule]]:\n",
    "        return [CommutativeSwapRule]\n",
    "\n",
    "\n",
    "env = CustomTimestepRewards()\n",
    "problem = \"4x + y + 2x\"\n",
    "expression = env.parser.parse(problem)\n",
    "state = MathyEnvState(problem=problem)\n",
    "\n",
    "_, transition, _ = env.get_next_state(\n",
    "    state, env.random_action(expression, AssociativeSwapRule),\n",
    ")\n",
    "# Expect positive reward\n",
    "assert transition.reward > 0.0\n",
    "\n",
    "_, transition, _ = env.get_next_state(\n",
    "    state, env.random_action(expression, CommutativeSwapRule),\n",
    ")\n",
    "# Expect neagative reward\n",
    "assert transition.reward < 0.0"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
