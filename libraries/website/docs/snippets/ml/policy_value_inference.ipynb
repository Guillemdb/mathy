{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This file is generated from a mathy documentation code snippet.\n",
    "!pip install mathy\n",
    "from mathy.envs.gym import MathyGymEnv\n",
    "from colr import color\n",
    "\n",
    "from mathy.cli import setup_tf_env\n",
    "from mathy.agents.a3c import A3CConfig\n",
    "from mathy.agents.action_selectors import A3CGreedyActionSelector\n",
    "from mathy.state import observations_to_window, MathyObservation, MathyEnvState\n",
    "from mathy.agents.policy_value_model import get_or_create_policy_model, PolicyValueModel\n",
    "from mathy.agents.episode_memory import EpisodeMemory\n",
    "from mathy.util import calculate_grouping_control_signal\n",
    "from mathy.envs import PolySimplify\n",
    "\n",
    "setup_tf_env()\n",
    "env = PolySimplify()\n",
    "args = A3CConfig(model_dir=\"/dev/null\", verbose=True,)\n",
    "episode_memory = EpisodeMemory()\n",
    "env_state: MathyEnvState = env.get_initial_state()[0]\n",
    "last_observation: MathyObservation = env.state_to_observation(\n",
    "    env_state, rnn_size=args.lstm_units\n",
    ")\n",
    "last_text = env_state.agent.problem\n",
    "last_action = -1\n",
    "last_reward = -1\n",
    "\n",
    "selector = A3CGreedyActionSelector(\n",
    "    model=get_or_create_policy_model(args=args, env_actions=env.action_size,),\n",
    "    episode=0,\n",
    "    worker_id=0,\n",
    ")\n",
    "\n",
    "# Set RNN to 0 state for start of episode\n",
    "selector.model.embedding.reset_rnn_state()\n",
    "\n",
    "# Start with the \"init\" sequence [n] times\n",
    "for i in range(args.num_thinking_steps_begin + 1):\n",
    "    rnn_state_h = selector.model.embedding.state_h.numpy()\n",
    "    rnn_state_c = selector.model.embedding.state_c.numpy()\n",
    "    seq_start = env_state.to_start_observation(rnn_state_h, rnn_state_c)\n",
    "    selector.model.call(observations_to_window([seq_start]).to_inputs())\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # store rnn state for replay training\n",
    "    rnn_state_h = selector.model.embedding.state_h.numpy()\n",
    "    rnn_state_c = selector.model.embedding.state_c.numpy()\n",
    "    last_rnn_state = [rnn_state_h, rnn_state_c]\n",
    "\n",
    "    # named tuples are read-only, so add rnn state to a new copy\n",
    "    last_observation = MathyObservation(\n",
    "        nodes=last_observation.nodes,\n",
    "        mask=last_observation.mask,\n",
    "        values=last_observation.values,\n",
    "        type=last_observation.type,\n",
    "        time=last_observation.time,\n",
    "        rnn_state=last_rnn_state,\n",
    "        rnn_history=episode_memory.rnn_weighted_history(args.lstm_units),\n",
    "    )\n",
    "    window = episode_memory.to_window_observation(last_observation)\n",
    "    action, value = selector.select(\n",
    "        last_state=env_state,\n",
    "        last_window=window,\n",
    "        last_action=last_action,\n",
    "        last_reward=last_reward,\n",
    "        last_rnn_state=last_rnn_state,\n",
    "    )\n",
    "    observation, reward, done, _ = env.step(env_state, action)\n",
    "\n",
    "    rnn_state_h = selector.model.embedding.state_h.numpy()\n",
    "    rnn_state_c = selector.model.embedding.state_c.numpy()\n",
    "\n",
    "    observation = MathyObservation(\n",
    "        nodes=observation.nodes,\n",
    "        mask=observation.mask,\n",
    "        values=observation.values,\n",
    "        type=observation.type,\n",
    "        time=observation.time,\n",
    "        rnn_state=[rnn_state_h, rnn_state_c],\n",
    "        rnn_history=episode_memory.rnn_weighted_history(args.lstm_units),\n",
    "    )\n",
    "\n",
    "    new_text = env_state.agent.problem\n",
    "    grouping_change = calculate_grouping_control_signal(\n",
    "        last_text, new_text, clip_at_zero=args.clip_grouping_control\n",
    "    )\n",
    "    episode_memory.store(\n",
    "        observation=last_observation,\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        grouping_change=grouping_change,\n",
    "        value=value,\n",
    "    )\n",
    "    if done:\n",
    "        # Last timestep reward\n",
    "        win = reward > 0.0\n",
    "        env.render_state(args.print_mode, None)\n",
    "        print(color(text=\"SOLVE\" if win else \"FAIL\", fore=\"green\" if win else \"red\",))\n",
    "        break\n",
    "\n",
    "    last_observation = observation\n",
    "    last_action = action\n",
    "    last_reward = reward"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
