Mathy includes a framework for building [reinforcement learning](/ml/reinforcement_learning) environments that transform math expressions using a set of user-defined actions. 

There are a number of built-in environments aimed at simplifying algebra problems, and generous customization points for creating new ones.

# Overview

Mathy environments are used to encapsulate all the changes that happen to an expression from its initial state until it reaches a desired output state. The environments follow a standard reinforcement learning environment lifecycle, which looks roughly like:

 1. set **state** to an **initial state** generated by the environment
 2. **while** **state** is not **terminal**
    - take an **action** and get a new **state**
 3. **done**

# Customization

Because algebra problems are only a tiny sliver of what can be represented using math expression trees, Mathy has customization points to allow altering or creating entirely new environments with little effort.

## Actions

Build your own tree transformation actions and use them with the built-in agents

## Problems

Generate arbitrary expressions for the agent to solve.

!!! note 

   Generating a new problem type while subclassing a base environment is probably the simplest way to create a custom challenge for the agent. You can inherit from a base environment like [Polynomial Simplification](/envs/polynomial_combine) which has win-conditions that require all the like-terms to be gone from an expression, and all complex terms be simplified.

## Timestep Rewards

Specify which actions the agent should be rewarded for using and which it should be penalized for. 


# Win Conditions

Environments can inherit win conditions from a base class or implement their own logic

# Episode Win/Loss Rewards

Specify (or calculate) custom floating point terminal reward values

